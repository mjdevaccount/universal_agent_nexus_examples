name: content-moderation-pipeline
version: "1.0.0"
description: "Multi-stage content moderation with AI risk assessment"

graphs:
  - name: moderate_content
    entry_node: risk_assessment
    nodes:
      - id: risk_assessment
        kind: router
        label: "AI Risk Classifier"
        router:
          name: risk_router
      
      - id: auto_approve
        kind: task
        label: "Auto Approve"
        config:
          action: "approve"
          status: "published"
      
      - id: policy_check
        kind: tool
        label: "Policy Validator"
        tool_ref: policy_validator
      
      - id: human_review
        kind: task
        label: "Escalate to Human Review"
        config:
          action: "escalate"
          queue: "moderation_queue"
      
      - id: auto_reject
        kind: task
        label: "Auto Reject"
        config:
          action: "reject"
          reason: "violates_policy"
    
    edges:
      # Use expression conditions with last_response (router output stored in context)
      - from_node: risk_assessment
        to_node: auto_approve
        condition:
          expression: "last_response.strip().lower() == 'safe'"
      
      - from_node: risk_assessment
        to_node: policy_check
        condition:
          expression: "last_response.strip().lower() == 'low'"
      
      - from_node: risk_assessment
        to_node: human_review
        condition:
          expression: "last_response.strip().lower() == 'medium'"
      
      - from_node: risk_assessment
        to_node: auto_reject
        condition:
          expression: "last_response.strip().lower() == 'high' or last_response.strip().lower() == 'critical'"
      
      - from_node: policy_check
        to_node: auto_approve
        condition:
          trigger: success
          expression: "result.compliant == true"
      
      - from_node: policy_check
        to_node: human_review
        condition:
          trigger: success
          expression: "result.compliant == false"

routers:
  - name: risk_router
    strategy: llm
    system_message: |
      You classify content risk. Respond with ONE word only: safe, low, medium, high, or critical.
      No explanations, no JSON, just the word.
    default_model: "ollama://qwen3:8b"

tools:
  - name: policy_validator
    description: "Validates content against community policies"
    protocol: "http"
    config:
      endpoint: "https://api.example.com/validate-policy"
      method: "POST"
      headers:
        Content-Type: "application/json"
    input_schema:
      type: object
      properties:
        content:
          type: string
        policy_version:
          type: string
    output_schema:
      type: object
      properties:
        compliant:
          type: boolean
        violations:
          type: array

policies: []

