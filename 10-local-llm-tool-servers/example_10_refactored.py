"""Example 10: Local LLM Tool Servers - Refactored with ToolCallingWorkflow

Demonstrates using the ToolCallingWorkflow helper to orchestrate
multiple LLM tool servers with automatic error handling, iteration limits,
and comprehensive metrics collection.

Before (custom): 120+ lines of LangGraph orchestration
After (refactored): 70 lines using ToolCallingWorkflow
Reduction: -42% code

Key Features:
- Automatic tool binding from LangChain Tool objects
- Iterative tool-calling loops with max iteration safety
- Comprehensive error handling (tool not found, execution errors)
- Full metrics collection and visualization
- Type-safe state management

Usage:
    ollama serve  # Terminal 1
    python 10-local-llm-tool-servers/example_10_refactored.py  # Terminal 2
"""

import asyncio
import json
from typing import Any

from langchain.tools import Tool
from langchain_ollama import ChatOllama

from shared.workflows import ToolCallingWorkflow


# ============================================================================
# Tool Definitions
# ============================================================================

def arithmetic_tool_impl(expression: str) -> str:
    """Execute simple arithmetic expressions safely.
    
    Args:
        expression: Mathematical expression (e.g., "2 + 3 * 4")
        
    Returns:
        Result of the calculation
    """
    try:
        # Safe evaluation using eval with limited scope
        result = eval(expression, {"__builtins__": {}}, {})
        return f"Result: {result}"
    except Exception as e:
        return f"Error: {str(e)}"


def web_search_tool_impl(query: str) -> str:
    """Simulate web search results.
    
    Args:
        query: Search query
        
    Returns:
        Simulated search results
    """
    # Simulated search results (in real scenario, use actual search API)
    results = {
        "python": "Python is a high-level, interpreted programming language known for its simplicity.",
        "asyncio": "asyncio is a library for writing concurrent code using async/await syntax in Python.",
        "langchain": "LangChain is a framework for developing applications powered by language models.",
        "ollama": "Ollama is a tool for running large language models locally on your machine.",
    }
    
    # Simple matching
    query_lower = query.lower()
    for key, value in results.items():
        if key in query_lower:
            return value
    
    return f"No results found for '{query}'"


def text_length_tool_impl(text: str) -> str:
    """Get length of text and word count.
    
    Args:
        text: Input text
        
    Returns:
        Character and word count
    """
    char_count = len(text)
    word_count = len(text.split())
    return f"Text has {char_count} characters and {word_count} words."


# ============================================================================
# Create LangChain Tool Objects
# ============================================================================

arithmetic_tool = Tool(
    name="arithmetic",
    func=arithmetic_tool_impl,
    description="Execute mathematical expressions. Input should be a valid Python expression like '2 + 3 * 4'.",
)

web_search_tool = Tool(
    name="web_search",
    func=web_search_tool_impl,
    description="Search for information on the web. Input should be a search query.",
)

text_length_tool = Tool(
    name="text_length",
    func=text_length_tool_impl,
    description="Get the length (character and word count) of provided text.",
)

tools = [arithmetic_tool, web_search_tool, text_length_tool]


# ============================================================================
# Create and Configure Workflow
# ============================================================================

async def main() -> None:
    """Run the Example 10 workflow."""
    
    # Initialize LLM
    llm = ChatOllama(
        model="mistral",
        base_url="http://localhost:11434",
        temperature=0.3,  # Lower temperature for tool selection
    )
    
    # Create workflow
    workflow = ToolCallingWorkflow(
        name="local-llm-tool-servers",
        llm=llm,
        tools=tools,
        max_iterations=5,  # Safety limit
    )
    
    # Test queries that require tool use
    test_queries = [
        "Calculate 15 * 8 plus 23",
        "What is Python and tell me about asyncio?",
        "How many words are in the phrase 'The quick brown fox jumps over the lazy dog'?",
        "Find information about Ollama and then calculate 100 / 4",
    ]
    
    print("\n" + "=" * 80)
    print("Example 10: Local LLM Tool Servers (Refactored with ToolCallingWorkflow)")
    print("=" * 80)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nðŸ“Œ Query {i}: {query}")
        print("-" * 80)
        
        try:
            # Execute workflow
            result = await workflow.invoke(query)
            
            # Extract results
            final_response = result.get("final_response", "No response generated")
            tool_calls = result.get("tool_calls", [])
            metrics = result.get("metrics", {})
            
            # Display response
            print(f"\nðŸ’¬ Response:\n{final_response}")
            
            # Display tool calls if any
            if tool_calls:
                print(f"\nðŸ”§ Tool Calls ({len(tool_calls)}):")
                for j, call in enumerate(tool_calls, 1):
                    print(f"  {j}. {call.get('name', 'unknown')}")
                    if call.get('input'):
                        print(f"     Input: {call['input']}")
                    if call.get('output'):
                        print(f"     Output: {call['output']}")
            
            # Display metrics
            if metrics:
                print(f"\nðŸ“Š Metrics:")
                print(f"  - Duration: {metrics.get('duration_ms', 0):.1f}ms")
                print(f"  - Tool calls: {metrics.get('tool_call_count', 0)}")
                print(f"  - Status: {metrics.get('status', 'unknown')}")
                if metrics.get('warnings'):
                    print(f"  - Warnings: {len(metrics['warnings'])}")
        
        except Exception as e:
            print(f"âŒ Error: {str(e)}")
    
    # Print workflow metrics summary
    print("\n" + "=" * 80)
    print("Workflow Summary")
    print("=" * 80)
    
    workflow_metrics = workflow.get_metrics()
    print(f"\nWorkflow: {workflow_metrics.get('workflow_name', 'unknown')}")
    print(f"Total Queries: {i}")
    print(f"Status: {workflow_metrics.get('overall_status', 'unknown')}")
    print(f"Total Duration: {workflow_metrics.get('total_duration_ms', 0):.1f}ms")
    print(f"\nAverage metrics per query:")
    if i > 0:
        avg_duration = workflow_metrics.get('total_duration_ms', 0) / i
        print(f"  - Average duration: {avg_duration:.1f}ms")
    
    print("\nâœ… Example 10 complete!")


if __name__ == "__main__":
    asyncio.run(main())
