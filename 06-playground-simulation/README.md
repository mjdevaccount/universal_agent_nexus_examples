# ğŸ® Interactive Agent Playground

**Watch agents with different personalities interact in real-time.**

## ğŸ’° 100% FREE - No API Keys Needed!

This playground runs **completely free** using [Ollama](https://ollama.com) for local LLM inference. No OpenAI. No paid services. Runs offline on your laptop.

---

## ğŸš€ Quick Start (2 Minutes)

### 1. Install Ollama

```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows: Download from https://ollama.com/download
```

### 2. Pull a Model (4GB)

```bash
ollama pull llama3.2:3b    # Best quality/speed balance
```

### 3. Install Fabric CLI

```bash
pip install fabric
fabric --setup             # Select Ollama
```

### 4. Run the Playground

```bash
cd 06-playground-simulation/backend
pip install -r requirements.txt
uvicorn main:app --reload
```

### 5. Open Frontend

Open `frontend/index.html` in your browser.

**Select agents â†’ Click "Run Simulation" â†’ Watch them talk! ğŸ‰**

```
Bully: "This is MY swing! Get lost!"
Shy Kid: "um... okay... sorry..."
Mediator: "Hey, maybe we can take turns?"
Joker: "Why did the swing break up with the slide? Too much drama! ğŸ˜‚"
Teacher: "Everyone gets a turn. Let's be kind."
```

**Zero API costs. Zero vendor lock-in. Runs on any laptop.**

---

## ğŸ—ï¸ Architecture

```
Universal Agent Fabric (Role Definitions)
    â”œâ”€ fabric_archetypes/*.yaml
    â”œâ”€ ontology/capabilities/
    â”œâ”€ ontology/domains/
    â””â”€ policy/rules/
         â†“
Playground Backend (FastAPI + WebSocket)
         â†“
danielmiessler Fabric CLI
         â†“
Ollama (Local LLM) â† 100% FREE!
```

---

## ğŸ“Š Performance (Local Ollama)

| Model | VRAM | Speed | Quality |
|-------|------|-------|---------|
| `llama3.2:1b` | 1GB | 80 t/s | Good for demos |
| `llama3.2:3b` | 2GB | 50 t/s | âœ… **Recommended** |
| `phi3:mini` | 2GB | 60 t/s | Excellent reasoning |
| `gemma2:2b` | 1.5GB | 70 t/s | Runs on Raspberry Pi |

**Response time:** ~1-2 seconds per agent turn

---

## ğŸ­ Agent Archetypes

| Archetype | Personality | Base Template |
|-----------|-------------|---------------|
| **The Bully** ğŸ’ª | Aggressive, dominant | react_loop |
| **The Shy Kid** ğŸ˜° | Timid, apologetic | simple_response |
| **The Mediator** ğŸ¤ | Diplomatic, problem-solver | planning_loop |
| **The Joker** ğŸ˜„ | Humorous, defuses tension | simple_response |
| **The Teacher** ğŸ‘¨â€ğŸ« | Authoritative, kind | react_loop |

---

## ğŸ”§ Configuration

### Switch Models

```bash
# Faster (lower quality)
fabric --set-default-model llama3.2:1b

# Better quality
fabric --set-default-model llama3.2:3b

# Best reasoning
fabric --set-default-model phi3:mini
```

### Run Offline

```bash
# After initial setup, no internet needed
ollama serve  # Runs locally
```

---

## ğŸ“ Project Structure

```
06-playground-simulation/
â”œâ”€â”€ fabric_archetypes/           # Role definitions (YOUR Fabric)
â”‚   â”œâ”€â”€ bully.yaml
â”‚   â”œâ”€â”€ shy_kid.yaml
â”‚   â”œâ”€â”€ mediator.yaml
â”‚   â”œâ”€â”€ joker.yaml
â”‚   â””â”€â”€ teacher.yaml
â”œâ”€â”€ ontology/
â”‚   â”œâ”€â”€ capabilities/            # What agents can do
â”‚   â””â”€â”€ domains/                 # Capability groupings
â”œâ”€â”€ policy/
â”‚   â””â”€â”€ rules/                   # Safety guardrails
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                  # FastAPI server
â”‚   â”œâ”€â”€ llm_provider.py          # Ollama/Fabric integration
â”‚   â”œâ”€â”€ fabric_compiler.py       # FabricBuilder integration
â”‚   â””â”€â”€ schemas.py               # Pydantic models
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html               # Interactive UI
â””â”€â”€ tests/
    â””â”€â”€ test_fabric_integration.py  # 16 passing tests
```

---

## ğŸ› ï¸ Customization

### Add Your Own Archetype

```yaml
# fabric_archetypes/inventor.yaml
name: "The Inventor"
base_template: "planning_loop"
system_prompt_template: |
  You are creative and curious. Propose novel solutions.
  Think outside the box. Keep responses SHORT.
default_capabilities:
  - "speak"
  - "brainstorm"
```

Then add to `ARCHETYPES` dict in `backend/main.py`.

### Change Scenarios

Try:
- "Group project where one person isn't working"
- "Choosing teams for kickball"
- "A new kid trying to join the group"

---

## ğŸ”„ Fallback Options

The `llm_provider.py` automatically detects available backends:

1. **danielmiessler Fabric CLI** â†’ Uses Ollama (FREE) âœ…
2. **Direct OpenAI** â†’ Fallback if Fabric not installed (requires API key)

```python
# From llm_provider.py
if self.fabric_available:
    return await self._complete_with_fabric(...)  # FREE with Ollama
else:
    return await self._complete_with_openai(...)  # Paid fallback
```

---

## ğŸ§ª Run Tests

```bash
cd 06-playground-simulation
pytest tests/ -v  # 16 tests, all passing
```

---

## ğŸ“š More Documentation

- [DEVELOPER_GUIDE.md](DEVELOPER_GUIDE.md) - How to extend the playground
- [UPSTREAM_GAPS.md](UPSTREAM_GAPS.md) - Library improvement opportunities

---

## ğŸ¤ Contributing

Want to add a cool archetype? Submit a PR!

Ideas:
- The Inventor (creative, curious)
- The Athlete (competitive, energetic)
- The Artist (expressive, emotional)
- The Scientist (analytical, logical)

---

**Built with:**
- [Universal Agent Fabric](https://github.com/mjdevaccount/universal_agent_fabric)
- [danielmiessler Fabric](https://github.com/danielmiessler/fabric)
- [Ollama](https://ollama.com) - **FREE local inference**

**Try other examples:** [01-hello-world](../01-hello-world/) â€¢ [02-content-moderation](../02-content-moderation/)
